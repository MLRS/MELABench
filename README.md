# MELABench: A Maltese Evaluation Language Benchmark ðŸ‡²ðŸ‡¹

To run evaluation on this benchmark, we provide code to do this in various ways:
- [Prompting](prompting): runs models by prompting them with pre-defined instructions.
- [Fine-Tuning](finetuning): trains models first before evaluating them.
